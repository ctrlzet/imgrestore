{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Data preprocessing"
      ],
      "metadata": {
        "id": "WDIrp159AfAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Firsty, create a shortcut of this [shared directory](https://drive.google.com/drive/folders/1LYgWpfDm3-A4q_QVcPGupsaNjblfCfYZ) in your Google Drive root: *MyDrive*\n",
        "\n",
        "2. Specify path to shortcut as `src_path` in cell below, eg. we stored it in *MyDrive/shortcuts/*\n",
        "\n",
        "3. Leave `target_path` as it is\n",
        "\n",
        "4. Run cell below"
      ],
      "metadata": {
        "id": "nhq9VwY-jMb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "print('Mounting Google Drive...')\n",
        "drive.mount('/gdrive')\n",
        "\n",
        "src_path = '/gdrive/MyDrive/shortcuts/' #@param {type: 'string'}\n",
        "assert os.path.exists(src_path), f\"Source '{src_path}' doesn't exist!\"\n",
        "\n",
        "target_path = '.' #@param {type: 'string'}\n",
        "os.makedirs(target_path, exist_ok=True)\n",
        "assert os.path.exists(target_path), f\"Target '{target_path}' doesn't exist!\"\n",
        "\n",
        "target_path = os.path.join(target_path, os.path.basename(src_path))\n",
        "print(f'Copying from \"{src_path}\" to \"{target_path}\"...')\n",
        "os.makedirs(target_path, exist_ok=True)\n",
        "!cp -rf \"$src_path\"/* \"$target_path\"  # also work when source is a shortcut"
      ],
      "metadata": {
        "id": "CRU7vVexvKZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb13fe11-0037-4fab-db02-56f625d0e8a0",
        "cellView": "form"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounting Google Drive...\n",
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n",
            "Copying from \"/gdrive/MyDrive/shortcuts/\" to \"./\"...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip it to transfer it from GDrive to Colab:"
      ],
      "metadata": {
        "id": "Z4RrFUHzkCIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!sudo tar -xvf project/*\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "!pip install transformers wandb timm"
      ],
      "metadata": {
        "id": "Az2bMRoYv9IH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.1. Imports"
      ],
      "metadata": {
        "id": "1VG6XOUokbjY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch\n",
        "import torchvision.transforms as TT\n",
        "import torchvision\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Custom decorators\n",
        "from transformers import TrainingArguments, Trainer, TrainerCallback\n",
        "\n",
        "# Metrics(network_swinir, util_calculate_psnr_ssim should be placed in root dir)\n",
        "from network_swinir import SwinIR\n",
        "from util_calculate_psnr_ssim import calculate_psnr, calculate_ssim\n",
        "\n",
        "# Service imports\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Plotting, reading, etc.\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from termcolor import colored\n",
        "\n",
        "# GUI for metrics\n",
        "import wandb\n",
        "\n",
        "# numpy for life\n",
        "import numpy as np\n",
        "\n",
        "sys.path.append(os.path.join(os.path.dirname(sys.path[1]),'gdrive', 'MyDrive', 'ML_proj'))\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "ebzwWM8bzrXC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de289e4-3c61-4fad-cd6a-825340648c85"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0.2 Data preparation"
      ],
      "metadata": {
        "id": "2fpPkXRHkk-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_path = 'BSR/BSDS500/data/images/train/'\n",
        "test_path = 'BSR/BSDS500/data/images/test/'\n",
        "val_path = 'BSR/BSDS500/data/images/test/'"
      ],
      "metadata": {
        "id": "ckKKvnmxztmS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = 432\n",
        "VAL_SIZE = 68"
      ],
      "metadata": {
        "id": "gfbCsFM3zuYI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extracting data to a comfortable representation in code"
      ],
      "metadata": {
        "id": "FQft7_Jhi2XT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_pics_list = []\n",
        "\n",
        "for pic_name in os.listdir(train_path):\n",
        "    if pic_name.split('.')[-1] == 'jpg':\n",
        "        train_pics_list.append(train_path + pic_name)\n",
        "\n",
        "for pic_name in os.listdir(test_path):\n",
        "    if pic_name.split('.')[-1] == 'jpg':\n",
        "        train_pics_list.append(test_path + pic_name)\n",
        "\n",
        "val_pics_for_train = []\n",
        "\n",
        "for pic_name in os.listdir(val_path):\n",
        "\n",
        "    if len(val_pics_for_train) == TRAIN_SIZE - len(train_pics_list):\n",
        "        break\n",
        "    \n",
        "    if pic_name.split('.')[-1] == 'jpg':\n",
        "        val_pics_for_train.append(val_path + pic_name)\n",
        "\n",
        "train_pics_list += val_pics_for_train\n",
        "\n",
        "assert len(train_pics_list) == TRAIN_SIZE\n",
        "\n",
        "val_pics_list = []\n",
        "\n",
        "remaining_pics_list = []\n",
        "\n",
        "for pic_name in os.listdir(val_path):\n",
        "\n",
        "    if len(val_pics_list) == VAL_SIZE:\n",
        "        break\n",
        "    \n",
        "    if pic_name.split('.')[-1] == 'jpg':\n",
        "        val_pics_list.append(val_path + pic_name)\n",
        "\n",
        "assert len(val_pics_list) == VAL_SIZE"
      ],
      "metadata": {
        "id": "igWagdqNzuaE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we introduce our custom classes to make our code more suitable for paper's logics"
      ],
      "metadata": {
        "id": "Ru09GentmPm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageNoiseAdditor(object):\n",
        "    def __init__(self, train_params, valid_params):\n",
        "        '''\n",
        "        train_params = dict(sigma_from: int [from 0 to 255], \n",
        "                            sigma_to: int [from 0 to 255])\n",
        "\n",
        "        valid_params = dict(sigma: int(from 0 to 255))\n",
        "        '''\n",
        "        self.train_params = train_params\n",
        "        self.valid_params = valid_params\n",
        "\n",
        "\n",
        "    def apply(self, image, train=True):\n",
        "\n",
        "        assert isinstance(image, Image.Image), colored(f'wrong type of image (wait {Image.Image})', 'red') + f' Got:{type(image)}'\n",
        "        img_numpy = np.asarray(image).astype(float)\n",
        "        \n",
        "        if train:\n",
        "            sigma = np.random.randint(low=self.train_params['sigma_from'], \n",
        "                                      high=self.train_params['sigma_to'])\n",
        "        else:\n",
        "            sigma = self.valid_params['sigma']\n",
        "        \n",
        "        n = np.random.randn(*img_numpy.shape)\n",
        "        img_numpy += float(sigma) * n\n",
        "        img_numpy = np.clip(img_numpy, 0, 255).astype(np.uint8)\n",
        "\n",
        "        image_with_noise = Image.fromarray(img_numpy, mode='RGB')\n",
        "\n",
        "        return image_with_noise, sigma"
      ],
      "metadata": {
        "id": "t65xGePCwG-6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BSDDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, img_list, transform, height, width, sigma_val=50, train=False):\n",
        "    super(BSDDataset, self).__init__()\n",
        "    self.img_list = img_list\n",
        "    self.transform = transform\n",
        "    self.height = height\n",
        "    self.width = width\n",
        "    self.to_tensor = TT.Compose([TT.Resize((height, width)),\n",
        "                                 TT.ToTensor()])\n",
        "    \n",
        "    self.noise = ImageNoiseAdditor(train_params=dict(sigma_from=10, sigma_to=55),\n",
        "                                   valid_params=dict(sigma=sigma_val))\n",
        "    \n",
        "    self.train = train\n",
        "\n",
        "  def get_rand_from_to(self, from_, to_):\n",
        "      return np.random.randint(low=int(from_), high=int(to_))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_list)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    img = Image.open(self.img_list[idx])\n",
        "    img_with_noise, noise_val = self.noise.apply(img, train=self.train)\n",
        "\n",
        "    if np.random.choice([True, False]):\n",
        "       img = torchvision.transforms.functional.hflip(img)\n",
        "       img_with_noise = torchvision.transforms.functional.hflip(img_with_noise)\n",
        "\n",
        "    w, h = img.size[:2]\n",
        "\n",
        "    max_top = h / 2\n",
        "    max_left = w / 2\n",
        "\n",
        "    if h - self.height > 0:\n",
        "        max_top = h - self.height\n",
        "        top = self.get_rand_from_to(0, max_top)\n",
        "        height = self.get_rand_from_to(self.height, h - top)\n",
        "    else:\n",
        "        top = self.get_rand_from_to(0, max_top)\n",
        "        height = self.get_rand_from_to(h / 2, h - top)\n",
        "\n",
        "    if w - self.width > 0:\n",
        "        max_left = w - self.width\n",
        "        left = self.get_rand_from_to(0, max_left)\n",
        "        width = self.get_rand_from_to(self.width, w - left)\n",
        "    else:\n",
        "        left = self.get_rand_from_to(0, max_left)\n",
        "        width = self.get_rand_from_to(w / 2, w - left)\n",
        "\n",
        "    img = torchvision.transforms.functional.crop(img, top, left, height, width)\n",
        "    img_with_noise = torchvision.transforms.functional.crop(img_with_noise, top, left, height, width)\n",
        "\n",
        "    \n",
        "    return dict(inputs=self.to_tensor(img_with_noise), \n",
        "                labels=self.to_tensor(img),\n",
        "                sigma=torch.tensor(noise_val).float())"
      ],
      "metadata": {
        "id": "tbcx27jIwCFM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Training"
      ],
      "metadata": {
        "id": "udMNjFfakuzp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bringing images to a general representation, applying noise for training and packing it to dataloaders:"
      ],
      "metadata": {
        "id": "CGHFBzENmhTd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "upscale = 8\n",
        "window_size = 8\n",
        "height = (512 // upscale // window_size + 1) * window_size\n",
        "width = (512 // upscale // window_size + 1) * window_size\n",
        "\n",
        "random_transform = [\n",
        "    TT.Compose([TT.RandomCrop(320, padding=0),\n",
        "                        TT.Pad(2, padding_mode='reflect')]),\n",
        "    TT.Compose([TT.RandomCrop(320, padding=0), \n",
        "            ]),\n",
        "]\n",
        "\n",
        "train_transform = TT.Compose([\n",
        "    TT.Resize((height, width)),\n",
        "    TT.ToTensor()\n",
        "])\n",
        "\n",
        "val_transform = TT.Compose([\n",
        "    TT.Resize((height, width)),\n",
        "    TT.ToTensor()\n",
        "])\n",
        "\n",
        "train_set = BSDDataset(train_pics_list, transform=train_transform, \n",
        "                       height=height, width=width, train=True)\n",
        "\n",
        "val_set = BSDDataset(val_pics_list, transform=val_transform, \n",
        "                     height=height, width=width, sigma_val=50, train=True)\n",
        "\n",
        "\n",
        "print('Train size', len(train_set))\n",
        "print('Test size', len(val_set))"
      ],
      "metadata": {
        "id": "retNSnnyz5G8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bdeacf7f-33ed-4b60-9205-29bcf4c39aae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size 432\n",
            "Test size 68\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = SwinIR(upscale=1, img_size=(height, width),\n",
        "               window_size=window_size, img_range=1.,\n",
        "               depths=[6, 6, 6, 6], embed_dim=120, num_heads=[6, 6, 6, 6], \n",
        "               mlp_ratio=4, upsampler=None, resi_connection='3conv')"
      ],
      "metadata": {
        "id": "CQlTm3OoFOFE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23943d7a-8753-463f-9e51-3281aa55db66"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class  CharbonnierLoss(nn.Module):\n",
        "    def __init__(self, eps=1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.l2 = torch.nn.MSELoss()\n",
        "\n",
        "    def forward(self, I_pred, I_traget):\n",
        "        x = torch.sqrt(self.l2(I_pred, I_traget) + self.eps ** 2)\n",
        "        return x"
      ],
      "metadata": {
        "id": "E3cHZ8JJeuzg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.5 * 1e-5\n",
        "epochs = 400\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
        "                                                       T_max=epochs, \n",
        "                                                       eta_min=0, \n",
        "                                                       last_epoch=- 1, \n",
        "                                                       verbose=False)\n",
        "\n",
        "criterion = CharbonnierLoss()"
      ],
      "metadata": {
        "id": "QPio1wTQewpd"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(EvalPredict):\n",
        "    \n",
        "    predictions, label_ids = EvalPredict\n",
        "    batch_size = predictions.shape[0]\n",
        "    psnr_accumulator = np.zeros(batch_size)\n",
        "    ssim_accumulator = np.zeros(batch_size)\n",
        "\n",
        "    for b in range(batch_size):\n",
        "\n",
        "        output = predictions[b]\n",
        "        img_gt = label_ids[b]\n",
        "\n",
        "        output = (output * 255.0).round().astype(np.uint8).transpose(1, 2, 0)\n",
        "        img_gt = (img_gt * 255.0).round().astype(np.uint8).transpose(1, 2, 0)\n",
        "\n",
        "        psnr = calculate_psnr(output, img_gt, crop_border=0, input_order='HWC')\n",
        "        psnr_accumulator[b] = psnr\n",
        "\n",
        "        ssim = calculate_ssim(output, img_gt, crop_border=0, input_order='HWC')\n",
        "        ssim_accumulator[b] = ssim\n",
        "\n",
        "    return {'psnr': psnr_accumulator.mean(), 'ssim': ssim_accumulator.mean()}\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, loss, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss = loss\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        inp_, labels = inputs['inputs'], inputs['labels']\n",
        "        # forward pass\n",
        "        outputs = model(inp_)\n",
        "\n",
        "        # compute custom loss (suppose one has 3 labels with different weights)\n",
        "        loss = self.loss(outputs, labels)\n",
        "        return (loss, outputs) if return_outputs else loss"
      ],
      "metadata": {
        "id": "NFzb64Xie7yu"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelTestCallback(TrainerCallback):\n",
        "    def __init__(self, eval_data_set, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.eval_data_set = eval_data_set\n",
        "\n",
        "    def on_evaluate(self, args, state, control, **kwargs): # on_evaluate, on_epoch_begin\n",
        "        print(colored('Callback', 'green', attrs=['bold']))\n",
        "        caption = ['input', 'target', 'output']\n",
        "        # print(kwargs)\n",
        "        s_list = [15, 25, 50]\n",
        "        for s in s_list:\n",
        "            self.eval_data_set.noise.valid_params['sigma'] = s\n",
        "            idx = np.random.randint(low=0, high=len(self.eval_data_set))\n",
        "            data = self.eval_data_set.__getitem__(idx)\n",
        "            \n",
        "            inp, lbl = data['inputs'].to('cpu'), data['labels'].to('cpu')\n",
        "            model = kwargs['model'].to('cpu')\n",
        "            out = model(torch.unsqueeze(inp, dim=0))\n",
        "            \n",
        "            inp = [Image.fromarray((inp.cpu().detach().numpy().transpose(1, 2, 0) * 255).astype(np.uint8))]\n",
        "            lbl = [Image.fromarray((lbl.cpu().detach().numpy().transpose(1, 2, 0) * 255).astype(np.uint8))]\n",
        "            out = [Image.fromarray((out[0].cpu().detach().numpy().transpose(1, 2, 0) * 255).astype(np.uint8))]\n",
        "            \n",
        "            wandb.log({f\"examples sigma = {self.eval_data_set.noise.valid_params['sigma']}\": [wandb.Image(img, caption=caption[i]) for i, img in enumerate(inp + lbl + out)]})\n",
        "\n",
        "            del inp, lbl, out\n",
        "\n",
        "\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        kwargs['model'].to(device)"
      ],
      "metadata": {
        "id": "uuZCKyfvkh51"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 6\n",
        "batch_accumulation = 8\n",
        "path_for_save = os.path.join(os.path.dirname(sys.path[1]),'gdrive', 'MyDrive', 'ML_proj', 'checkpoints')\n",
        "\n",
        "args = TrainingArguments(output_dir=path_for_save,\n",
        "                         overwrite_output_dir=True,\n",
        "                         report_to='wandb',\n",
        "                         evaluation_strategy='steps',\n",
        "                         logging_steps=5,\n",
        "                         gradient_accumulation_steps=batch_accumulation,\n",
        "                         per_device_train_batch_size=batch_size,\n",
        "                         num_train_epochs=epochs,\n",
        "                         load_best_model_at_end=True,\n",
        "                         metric_for_best_model='ssim',\n",
        "                         greater_is_better=True,\n",
        "                         no_cuda=False,\n",
        "                         save_strategy='steps',\n",
        "                         save_steps=5, \n",
        "                         save_total_limit=3,\n",
        "                         ignore_data_skip=True,\n",
        "                         resume_from_checkpoint=os.path.join(os.path.dirname(sys.path[1]),'gdrive', 'MyDrive', 'ML_proj', 'checkpoints', 'checkpoint-1220')\n",
        "                         )\n",
        "\n",
        "trainer = CustomTrainer(loss=criterion,\n",
        "                        model=model, \n",
        "                        train_dataset=train_set,\n",
        "                        eval_dataset=val_set,\n",
        "                        compute_metrics=compute_metrics,\n",
        "                        optimizers=(optimizer, scheduler),\n",
        "                        callbacks=[ModelTestCallback(eval_data_set=val_set)],\n",
        "                        args=args)"
      ],
      "metadata": {
        "id": "d_JMFu4LfB4h"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Start training:"
      ],
      "metadata": {
        "id": "EQyrDvlKfMV6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used **WandB** framework for tracing metrics, so you should register [here](https://wandb.ai/home) and open the highlighted link below(*appears once you started to execute cell*):"
      ],
      "metadata": {
        "id": "IdIuCpwRky6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426
        },
        "id": "-s_4byMQfEXd",
        "outputId": "82d528b4-4e87-4f28-fab8-e357325cc204"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 432\n",
            "  Num Epochs = 400\n",
            "  Instantaneous batch size per device = 6\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 48\n",
            "  Gradient Accumulation steps = 8\n",
            "  Total optimization steps = 3600\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mscalyvladimir\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.11"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220323_022943-7w7khj13</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/scalyvladimir/huggingface/runs/7w7khj13\" target=\"_blank\">/gdrive/MyDrive/ML_proj/checkpoints</a></strong> to <a href=\"https://wandb.ai/scalyvladimir/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-3435b262f1ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 if (\n\u001b[1;32m   1403\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1404\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1405\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m                 ):\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/file_utils.py\u001b[0m in \u001b[0;36mis_torch_tpu_available\u001b[0;34m()\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;31m# This test is probably enough, but just in case, we unpack a bit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch_xla\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"torch_xla.core\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/util.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mparent_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_find_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfullname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_get_spec\u001b[0;34m(cls, fullname, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mfind_spec\u001b[0;34m(self, fullname, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_path_stat\u001b[0;34m(path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Projection layer"
      ],
      "metadata": {
        "id": "iL4fQTCne0Sl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introducing training [projection layer](https://arxiv.org/abs/1711.07807)"
      ],
      "metadata": {
        "id": "LYl4OcCfm9Nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProjectionLayer(torch.nn.Module):\n",
        "    def __init__(self, img_size):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.alpha = nn.Parameter(torch.randn(1), requires_grad=True)\n",
        "        self.Nt = torch.prod(torch.tensor(self.img_size))\n",
        "        \n",
        "\n",
        "    def calculate_eps(self, sigma):\n",
        "        return torch.exp(self.alpha) * sigma * torch.sqrt(self.Nt - 1)\n",
        "\n",
        "    def forward(self, pred, inp, sigma):\n",
        "\n",
        "        eps = self.calculate_eps(sigma)\n",
        "\n",
        "        d = pred - inp\n",
        "        \n",
        "        denom = torch.max(torch.linalg.norm(d, dim=(1, 2, 3)), eps)\n",
        "\n",
        "        nom = torch.mul(eps, d.transpose(3, 0))\n",
        "\n",
        "        p = inp + torch.div(nom, denom).transpose(3, 0)\n",
        "        return p\n",
        "\n",
        "\n",
        "class SwinIRP(nn.Module):\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__()\n",
        "        self.swin_ir = SwinIR(**kwargs)\n",
        "        img_size = (*kwargs['img_size'], 3)\n",
        "        self.pj_layer = ProjectionLayer(img_size)\n",
        "\n",
        "    def forward(self, x, sigma):\n",
        "        x_ = self.swin_ir.forward(x)\n",
        "        x_ = self.pj_layer(x_, x, sigma)\n",
        "        return x_\n",
        "\n",
        "    def load_swin_model(self, checkpoint):\n",
        "      print(colored('checkpoint:', 'grey', attrs=['bold']), checkpoint)\n",
        "      state_dict = torch.load(os.path.join(checkpoint, 'pytorch_model.bin'), map_location=\"cpu\")\n",
        "      load_result = self.swin_ir.load_state_dict(state_dict, strict=False)\n",
        "      print(load_result)"
      ],
      "metadata": {
        "id": "6Q5l3XyJ8ekf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SwinIRP(upscale=1, img_size=(height, width),\n",
        "                window_size=window_size, img_range=1.,\n",
        "                depths=[6, 6, 6, 6], embed_dim=120, num_heads=[6, 6, 6, 6], \n",
        "                mlp_ratio=4, upsampler=None, resi_connection='3conv')"
      ],
      "metadata": {
        "id": "BuHcExt_aOqA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Introducing a separate loss function and negative PSNR metric for projection layer training"
      ],
      "metadata": {
        "id": "2g4DbB3eneFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharbonnierLoss(nn.Module):\n",
        "    def __init__(self, eps=1e-3):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.l2 = torch.nn.MSELoss()\n",
        "\n",
        "    def forward(self, I_pred, I_traget):\n",
        "        x = torch.sqrt(self.l2(I_pred, I_traget) + self.eps ** 2)\n",
        "        return x\n",
        "\n",
        "class NegativePSNR(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        Nt = torch.prod(torch.tensor(pred.shape[1:]))\n",
        "        p = torch.sqrt(Nt) * 255\n",
        "\n",
        "        return torch.sum((-20) * torch.log10(p / torch.linalg.norm(pred - target, dim=(1, 2, 3))))"
      ],
      "metadata": {
        "id": "cjGN33S_Nlrv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Empirical hyperparameters"
      ],
      "metadata": {
        "id": "aGeZhWwZnsuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 2e-5\n",
        "epochs = 400\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999))\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, \n",
        "                                                       T_max=epochs, \n",
        "                                                       eta_min=0, \n",
        "                                                       last_epoch=- 1, \n",
        "                                                       verbose=False)\n",
        "\n",
        "criterion = NegativePSNR()"
      ],
      "metadata": {
        "id": "e5imQ5q999tK"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(EvalPredict):\n",
        "    \n",
        "    predictions, label_ids = EvalPredict\n",
        "    batch_size = predictions.shape[0]\n",
        "    psnr_accumulator = np.zeros(batch_size)\n",
        "    ssim_accumulator = np.zeros(batch_size)\n",
        "\n",
        "    for b in range(batch_size):\n",
        "\n",
        "        output = predictions[b]\n",
        "        img_gt = label_ids[b]\n",
        "\n",
        "        output = (output * 255.0).round().astype(np.uint8).transpose(1, 2, 0)\n",
        "        img_gt = (img_gt * 255.0).round().astype(np.uint8).transpose(1, 2, 0)\n",
        "\n",
        "        psnr = calculate_psnr(output, img_gt, crop_border=0, input_order='HWC')\n",
        "        psnr_accumulator[b] = psnr\n",
        "\n",
        "        ssim = calculate_ssim(output, img_gt, crop_border=0, input_order='HWC')\n",
        "        ssim_accumulator[b] = ssim\n",
        "\n",
        "    return {'psnr': psnr_accumulator.mean(), 'ssim': ssim_accumulator.mean()}\n",
        "\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, loss, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.loss = loss\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        inp_, labels, sigmas = inputs['inputs'], inputs['labels'], inputs['sigma']\n",
        "        # forward pass\n",
        "        outputs = model(inp_, sigmas)\n",
        "\n",
        "        # compute custom loss (suppose one has 3 labels with different weights)\n",
        "        loss = self.loss(outputs, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "class ModelTestCallback(TrainerCallback):\n",
        "    def __init__(self, eval_data_set, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.eval_data_set = eval_data_set\n",
        "\n",
        "    def on_evaluate(self, args, state, control, **kwargs): # on_evaluate, on_epoch_begin\n",
        "        print(colored('Callback', 'green', attrs=['bold']))\n",
        "        caption = ['input', 'target', 'output']\n",
        "        \n",
        "        s_list = [15, 25, 50]\n",
        "        for s in s_list:\n",
        "            self.eval_data_set.noise.valid_params['sigma'] = s\n",
        "            idx = np.random.randint(low=0, high=len(self.eval_data_set))\n",
        "            data = self.eval_data_set.__getitem__(idx)\n",
        "            \n",
        "            \n",
        "            inp, lbl, sigma = data['inputs'].to('cpu'), data['labels'].to('cpu'), data['sigma'].to('cpu')\n",
        "            model = kwargs['model'].to('cpu')\n",
        "            out = model(torch.unsqueeze(inp, dim=0), sigma)\n",
        "            \n",
        "\n",
        "            inp = [Image.fromarray((inp.cpu().detach().numpy().transpose(1, 2, 0) * 255).astype(np.uint8))]\n",
        "            lbl = [Image.fromarray((lbl.cpu().detach().numpy().transpose(1, 2, 0) * 255).astype(np.uint8))]\n",
        "            out = [Image.fromarray((out[0].cpu().detach().numpy().transpose(1, 2, 0) * 255).astype(np.uint8))]\n",
        "            \n",
        "            wandb.log({f\"examples sigma = {sigma}\": [wandb.Image(img, caption=caption[i]) for i, img in enumerate(inp + lbl + out)]})\n",
        "\n",
        "            del inp, lbl, out\n",
        "\n",
        "\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        kwargs['model'].to(device)"
      ],
      "metadata": {
        "id": "gpvPLDjQCEHJ"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some extra hyperparams"
      ],
      "metadata": {
        "id": "HsVO9kmMoSOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 6\n",
        "batch_accumulation = 4\n",
        "path_for_save = os.path.join(os.path.dirname(sys.path[1]),'gdrive', 'MyDrive', 'ML_proj', 'checkpoints_proj')\n",
        "\n",
        "args = TrainingArguments(output_dir=path_for_save,\n",
        "                         overwrite_output_dir=True,\n",
        "                         report_to='wandb',\n",
        "                         evaluation_strategy='steps',\n",
        "                         logging_steps=5,\n",
        "                         gradient_accumulation_steps=batch_accumulation,\n",
        "                         per_device_train_batch_size=batch_size,\n",
        "                         num_train_epochs=epochs,\n",
        "                         load_best_model_at_end=True,\n",
        "                         metric_for_best_model='ssim',\n",
        "                         greater_is_better=True,\n",
        "                         no_cuda=False,\n",
        "                         save_strategy='steps',\n",
        "                         save_steps=5, \n",
        "                         save_total_limit=3,\n",
        "                         ignore_data_skip=True)\n",
        "\n",
        "trainer = CustomTrainer(loss=criterion,\n",
        "                        model=model, \n",
        "                        train_dataset=train_set,\n",
        "                        eval_dataset=val_set,\n",
        "                        compute_metrics=compute_metrics,\n",
        "                        optimizers=(optimizer, scheduler),\n",
        "                        callbacks=[ModelTestCallback(eval_data_set=val_set)],\n",
        "                        args=args)                        "
      ],
      "metadata": {
        "id": "iT-ef4MxVrI9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3a9e7d8-6747-4ca9-a2c9-4a74e9905f98"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "using `logging_steps` to initialize `eval_steps` to 5\n",
            "PyTorch: setting up devices\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you are to specify path to pretrained model which saved by default to your GDrive"
      ],
      "metadata": {
        "id": "VrZ8a7JifqPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_dir_path = 'checkpoint-435'# example"
      ],
      "metadata": {
        "id": "mcVCjyY0flHg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train(os.path.join(os.path.dirname(sys.path[1]),'gdrive', 'MyDrive', 'ML_proj', 'checkpoints_proj', checkpoint_dir_path))"
      ],
      "metadata": {
        "id": "fkdh_3dwKSMZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        },
        "outputId": "275734b7-4075-48eb-f4da-b27b5c33ebb5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading model from /gdrive/MyDrive/ML_proj/checkpoints_proj/checkpoint-435).\n",
            "***** Running training *****\n",
            "  Num examples = 432\n",
            "  Num Epochs = 400\n",
            "  Instantaneous batch size per device = 6\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 24\n",
            "  Gradient Accumulation steps = 4\n",
            "  Total optimization steps = 7200\n",
            "  Continuing training from checkpoint, will skip to saved global_step\n",
            "  Continuing training from epoch 24\n",
            "  Continuing training from global step 435\n",
            "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='438' max='7200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 438/7200 00:07 < 14:19:10, 0.13 it/s, Epoch 24.11/400]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-8fba73da6330>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'gdrive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MyDrive'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ML_proj'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'checkpoints_proj'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint_dir_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1398\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1399\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1400\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1402\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2000\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2001\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2002\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2004\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    154\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}